---
title: "CitiBike Trip and Weather Data Analysis"
author: "Jash Gaglani (jg1700)"
output:
  pdf_document:
    toc: no
    toc_depth: '3'
  html_document:
    highlight: pygments
    theme: cerulean
    code_folding: show
    toc: no
    toc_float: yes
    toc_depth: 3
    keep_md: yes
    md_extensions: +grid_tables
classoption: portrait
urlcolor: blue
linkcolor: blue
editor_options:
  chunk_output_type: inline
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
---

<style>
  .main-container {
    max-width: 1200px !important;
  }
</style>

---

```{r setup, echo=F,message=F,warning=F}  
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h')
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(GGally)
library(lubridate)
library(caret)
library(gbm)
library(tidyverse)
library(caret)
```

# Overview 
This is an analysis of the NYC CitiBike dataset to view how the different variables affect the duration of the bike rides and how different scenarios affect number of bike rides taken in a time period. I clubbed the 2019 Citibike dataset with weather data for that time period to augment our dataset. I also built a couple of models to predict the ride duration as it is directly proportional to how much a ride will cost. With such intelligence, a company can be better positioned to determine actions that can increase it's revenue stream.

With the joined datasets these are the hypothesis that I was able to test 
  * Number of rides will go down during the winter months.
  * Number of rides go down during odd hours like after midnight and before dawn.
  * Number of rides taken will significantly increase on weekends as compared to weekdays.
  * Young riders will have longer bike ride durations and ride more frequently.
  * Yearly subscribers will have more number of rides and higher ride durations.
  
I recommend reading till page 13 and skipping to the last page for conclusion.

# Data Processing 

## Data sources

We obtained data from the following sources:  

### 1. CitiBike trip dataset

Since 2013, a shared bicycle system known as CitiBike has been available in New York City. CitiBike makes a vast amount of data available regarding system usage. The available usage data provides a wealth of information I used to find valuable trends.

For each month since the systemâ€™s inception, there is a file containing details of (almost) every trip. There are currently 108 monthly data files for the New York City bikeshare system, spanning July 2013 through December 2019. I used data from the year 2019 as I wanted to do it for a time period before covid. 

Each file contains a line for every trip. The number of trips per month varies from as few as 200,000 during winter months in the system's early days to more than 2 million trips in summer. Because of the computational limitations which this presented, I created samples of 1/1000. The samples were created non-deterministically, by randomly selecteing 'r nrow(file)/1000' from the file.  

The column names for this are pretty much self explanatory.


```{r load-citibike-data, cache = TRUE}
citibike_2019 <- read.csv("citibike_2019.csv")
citibike_2019 <- citibike_2019[sample(nrow(citibike_2019), 20000),]
head(citibike_2019)
```

### 2. Central Park daily weather data

I obtained historical weather information for the year 2019 from the NCDC (National Climatic Data Center) by submitting an online request to https://www.ncdc.noaa.gov/cdo-web/search. Although the weather may vary slightly within New York City, I used just the data associated with the Central Park observations as proxy for the entire city's weather.

I believe that the above data provides a reasonable representation of the target population (all CitiBike rides) and the citywide weather.

```{r weather-data, cache = TRUE}
weather <- read.csv("2812766.csv",header=FALSE)
names(weather)<-weather[1,]
weather<-weather[-1,]
weather<-subset(weather[weather$NAME == "NY CITY CENTRAL PARK, NY US" & year(weather$DATE) == 2019,],)
attr_indexes <- names(weather) %>% grep("ATTR",x = .)
weather <- weather[,-attr_indexes]
weather <- weather[,-c(1:5)]
weather<-weather %>%mutate(across(c(where(is.character), -DATE), as.numeric))
weather$DATE<-as.Date(weather$DATE)
weather <- weather %>% select("DATE","PRCP","SNOW","SNWD","TMAX","TMIN","WT01","WDF2","WDF5","WSF2","WSF5","WT08")
weather[is.na(weather)] <- 0
head(weather)
```
```{r echo=FALSE}
rm(attr_indexes)
```
PRCP = Precipitation (mm or inches as per user preference, inches to hundredths on Daily Form pdf file)
SNOW = Snowfall (mm or inches as per user preference, inches to tenths on Daily Form pdf file)
SNWD = Snow depth (mm or inches as per user preference, inches on Daily Form pdf file
WT01 = Fog, ice fog, or freezing fog (may include heavy fog)
WT08 = Smoke or haze 
WDF2 = Direction of fastest 2-minute wind (degrees)
WDF5 = Direction of fastest 5-second wind (degrees)
WSF2 = Fastest 2-minute wind speed (miles per hour or meters per second as per user preference
WSF5 = Fastest 5-second wind speed (miles per hour or meters per second as per user preference)

## Data Exploration    

In this section, I examine selected individual variables from the CitiBike and Weather datasets.  These items require transformation and/or cleaning as there are missing values or outliers which impede analysis otherwise.

```{r}
print(paste0("The citibike data consists of ", nrow(citibike_2019), " data points spread across ", ncol(citibike_2019), " features"))
```

#### Examine variable **trip_duration**:    
The trip_duration is specified in seconds, but there are some outliers which may be incorrect, as the value for Max is quite high:  `r summary(citibike_2019$tripduration)["Max."]` seconds, or `r summary(citibike_2019$tripduration)["Max."]/60/60/24` days. I assumed that this data is bad, as nobody would willingly rent a bicycle for this period of time, given the fees that would be charged.  Here is a histogram of the original data distribution:
```{r}
plot(citibike_2019$tripduration/60) 
```

#### Delete cases with unreasonable trip_duration values
Let's assume that nobody would rent a bicycle for more than a specified timelimit (say, 3 hours), and drop any records which exceed this:
```{r drop-long-trips}
num_long_trips_removed<- nrow(citibike_2019[citibike_2019$tripduration > 7200,])
citibike_2019<-citibike_2019[citibike_2019$tripduration<=7200,]
print(paste0("Removed ", num_long_trips_removed, " trips of longer than 2 hours."))
```
```{r, echo=FALSE}
rm(num_long_trips_removed)
```

#### Examine birth_year
Other inconsistencies concern the collection of birth_year, from which we can infer the age of the participant. There are some months in which this value is omitted, while there are other months in which all values are populated.  However, there are a few records which suggest that the rider is a centenarian -- it seems highly implausible that someone born in the 1880s is cycling around Central Park -- but the data does have such anomalies.  Thus, a substantial amount of time was needed for detecting and cleaning such inconsistencies.

The birth year for some users is as old as `r summary(citibike_2019$birth.year)["Min."]`, which is not possible:
```{r birth-year}
summary(citibike_2019$birth.year)
citibike_2019$age <- 2019 - citibike_2019$birth.year
```

#### Remove trips associated with very old users (age>90)
```{r age-and-birth-year}
num_old_age_removed<- nrow(citibike_2019[citibike_2019$age>90,])
citibike_2019<-citibike_2019[citibike_2019$age<90,]
print(paste0("Removed ", num_old_age_removed, " trips of people older thatn 90 years"))
```

```{r, echo=FALSE}
rm(num_old_age_removed)
```

#### Compute distance between start and end stations 
```{r, cache=TRUE}
library(geosphere)
citibike_2019$distance <- distHaversine(citibike_2019[,6:7], citibike_2019[,10:11])
citibike_2019$dist.lat <- abs((citibike_2019$start.station.latitude - citibike_2019$end.station.latitude))
citibike_2019$dist.long <- abs((citibike_2019$start.station.longitude - citibike_2019$end.station.longitude))
summary(citibike_2019$distance)
```

## Data Wrangling
```{r pre-processing}
is_weekday = function(timestamp){
  lubridate::wday(timestamp, week_start = 1) < 6
}
```

## Feature Extraction
Add columns for the hour when the trip started, day of the week and if it was a week day
```{r feature_extraction, cache=TRUE}
citibike_2019$start_date<-as.Date(citibike_2019$starttime)
citibike_2019$Hour<-hour(citibike_2019$starttime)
citibike_2019$dayofweek <- as.factor(wday(citibike_2019$starttime))
citibike_2019$weekday<-as.factor(as.numeric(sapply(citibike_2019$starttime, is_weekday)))
head(citibike_2019 %>% select("Hour","dayofweek","weekday"))
```

## Convert into factor variables
```{r convert_factor}
citibike_2019$usertype<-as.factor(citibike_2019$usertype)
citibike_2019$gender<-as.factor(citibike_2019$gender)
```

## Convert trip duration from seconds to minutes
```{r sec_to_min}
citibike_2019$tripduration<-floor(citibike_2019$tripduration/60)
head(citibike_2019$tripduration)
```

## Join Citibike and Weather data
```{r join_data}
citibike_2019 <- citibike_2019 %>% inner_join(weather, by = c("start_date" = "DATE" ))
head(citibike_2019)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
load("data.Rdata")
```

# Data Analysis to answer questions (hypotheses)

## Number of rides taken during each month
Here we can clearly see a trend that number of rides taken during December, January and February are lower as compared to other seasons. This shows the seasonality of the rides i.e. number of rides during winters go down.

```{r monthly-rides, fig.width = 15, fig.height=10, error=FALSE}
  barplot(table(substring(citibike_2019$starttime, 6, 7)), col=rainbow(12), xlab="Months", ylab="# of rides taken", main="# of rides taken during each month of the year in 2019", ylim=c(0, 3000))
```

## Number of rides during each hour of the day
Here we can clearly see that the number of rides taken after midnight fall drastically and the number of rides go up only around 7 am.

```{r hourly-rides, fig.width = 15, fig.height=10, error=FALSE}

  barplot(table(substring(citibike_2019$starttime, 12, 13)), col=rainbow(24), xlab="Hours", ylab="# of rides taken", main="# of rides taken during each hour of the day in 2019", ylim=c(0, 2000))

```

## Number of rides depending on if it's a weekend
Here we can clearly see that the number of rides taken during weekends is a lot more than on weekdays.

```{r weekend-rides, fig.width = 15, fig.height=10, error=FALSE}
  barplot(table(citibike_2019$weekday), col=rainbow(12), xlab="Type of Day (1 = Weekend and 0 = Weekday)", ylab="# of rides taken", main="# of rides taken during weekends vs weekdays", ylim=c(0, 20000))
```

## Ride duration based on user type (Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member)
The first plot reveals something interesting and counter intuitive. Initially I thought annual members would have longer ride duration but the plot proves my hypothesis incorrect. It makes sense because people with a pass for a couple of days might be tourists or took the pass specifically to explore the city so they have higher ride durations. At the same time annual members are regular commuters and probably need it to commute to office or school and hence ride for a short duration.

The second plot shows that annual subscribers take lot more rides as compared to customers which makes sense.

```{r user-type, fig.width = 15, fig.height=10, error=FALSE}
  boxplot(citibike_2019$tripduration ~ citibike_2019$usertype, col=rainbow(2), xlab="Type of User", ylab="Trip duration", main="Duration of trips for Customer vs Subscriber")

  barplot(table(citibike_2019$usertype), col=rainbow(2), xlab="Type of User", ylab="# of rides taken", main="# of rides taken by Customer vs Subscriber", ylim=c(0, 20000))
```

## Ride duration and number of rides based on user age
The first plot is surprisingly uniform. I expected younger riders to ride for longer durations but old people also ride for equally long durations. 

Looking at the second graph we can conclude that young people use the bikes more frequently and hence they are the prime demographic the company could target for ad campaigns.


```{r user-age, fig.width = 15, fig.height=10, error=FALSE}
  boxplot(citibike_2019$tripduration ~ citibike_2019$age, col=rainbow(30), xlab="User Age", ylab="Trip duration", main="Duration of trips for riders of different ages")
  barplot(table(citibike_2019$age), col=rainbow(30), xlab="User Age", ylab="# of rides taken", main="# of rides taken by users of different ages", ylim=c(0, 2000))
  
```

# Prediction Models

I used a lot of models below to fit the data to get a good estimation for the ride durations. I got the lowest root mean squared error (RMSE) of 7.5 with Random Forest Regressor so to maintain a concise document I have only shown the code for it below. Due to the short nature of this course and not having access to high level computing resources I built a very basic version of these models below. But the RMSE can be significantly brought down by performing hyper-parameter tuning and having better computing resources to increase the model complexity.

## Correlations of individual trip data features

We can examine the correlations between variables to understand the relationship between variables, and also to help be alert to potential problems of multicollinearity.  Here we compute actual correlations between key variables. Here we compute the correlations between key variables on the individual CitiBike Trip data

```{r compute-correl-by-ride, echo=F, message=F, warning=F}
#### compute correlations
library(Hmisc)
library(corrplot)

colstouse <-setdiff(names(citibike_2019),c("starttime","start.station.id","end.station.id",
                                           "stoptime","start.station.name","end.station.name",
                                           "start.station.longitude","start_date",
                                           "end.station.latitude","end.station.longitude",
                                           "weekday","bikeid","birth.year","DATE"))
X_train<-citibike_2019[ , which(names(citibike_2019) %in% colstouse)]%>%mutate(across(c(where(is.factor)), as.numeric))

res2<-rcorr(as.matrix(X_train))
respearson=rcorr(as.matrix(X_train),type = "pearson")
resspearman=rcorr(as.matrix(X_train),type = "spearman")
res3 <- cor(as.matrix(X_train))
```

```{r act-correlations-by-ride, echo=FALSE, fig.width = 15, fig.height=10, error=FALSE}
#### actual correlations (not rank correlations)
  corrplot(corr = res3, type = "upper", outline = T,  order="original", 
           sig.level = 0.05, insig = "blank", addCoef.col = "black",
           title = "\nActual correlations on individual trip data",
           number.cex = 1.4, number.font = 1, number.digits = 2 )
```

## Train - Test Split
```{r train-test}
smp_size<- floor(0.8*nrow(citibike_2019))
set.seed(123)
train_index<-sample(seq_len(nrow(citibike_2019)), size = smp_size)
train_data <- citibike_2019[train_index,]
test_data <-  citibike_2019[- train_index,]
```

```{r echo=FALSE, warning=FALSE}
rm(train_index, num_long_trips_removed, smp_size)
```


## Random Forest Model
```{r}
colstouse <-setdiff(names(train_data),c("starttime","start.station.id","end.station.id",
                                           "stoptime","start.station.name","end.station.name",
                                           "start.station.longitude","start_date",
                                           "end.station.latitude","end.station.longitude",
                                           "weekday","bikeid","birth.year","DATE","SNOW","SNWD"))
X_train<-train_data[ , which(names(train_data) %in% colstouse)]
names(X_train)
```

```{r rf-model-itting, cache = TRUE, eval=FALSE}
library('randomForest')

control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3)

#Metric compare model is Root Mean Squared Error
metric <- "RMSE"
set.seed(123)

#Number randomly variable selected is mtry
mtry <- sqrt(ncol(X_train)-1)
tunegrid <- expand.grid(.mtry=mtry)
rfmodel <- train(tripduration ~ ., 
                      data = X_train,
                      method='rf', 
                      metric=metric, 
                      tuneGrid=tunegrid, 
                      trControl=control)
```

```{r}
rfmodel
```

```{r}
summary(rfmodel)
```


```{r, warning = F}
trellis.par.set(caretTheme())
densityplot(rfmodel, pch = "|")
```

### Evaluate Performance on the train data
```{r}
y_test<-train_data$tripduration
predicted = predict(rfmodel,train_data)
residuals = y_test - predicted
RMSE = sqrt(mean(residuals^2))
cat('The root mean square error of the train data is ', round(RMSE,3),'\n')
```

```{r}
y_test_mean = mean(y_test)
# Calculate total sum of squares
tss =  sum((y_test - y_test_mean)^2 )
# Calculate residual sum of squares
rss =  sum(residuals^2)
# Calculate R-squared
rsq  =  1 - (rss/tss)
cat('The R-square of the train data is ', round(rsq,3), '\n')
```

```{r, warning=FALSE}
options(repr.plot.width=8, repr.plot.height=4)
my_data = as.data.frame(cbind(predicted = predicted,
                            observed = y_test))
# Plot predictions vs test data
ggplot(my_data,aes(predicted, observed)) + geom_point(color = "darkred", alpha = 0.5) + 
    geom_smooth(method=lm)+ ggtitle('Linear Regression ') + ggtitle("Random Forest: Prediction vs Test Data") +
      xlab("Predecited Trip Duration in Seconds ") + ylab("Observed Trip Duration in Secons") + 
        theme(plot.title = element_text(size=16,hjust = 0.5),
         axis.text.y = element_text(size=12), axis.text.x = element_text(size=12,hjust=.5),
         axis.title.x = element_text(size=14), axis.title.y = element_text(size=14))
```

### Evaluate Performance on the test data

```{r}
y_test<-test_data$tripduration
predicted = predict(gbmmodel,test_data)
residuals = y_test - predicted
RMSE = sqrt(mean(residuals^2))
cat('The root mean square error of the test data is ', round(RMSE,3),'\n')
```

```{r}
y_test_mean = mean(y_test)
# Calculate total sum of squares
tss =  sum((y_test - y_test_mean)^2 )
# Calculate residual sum of squares
rss =  sum(residuals^2)
# Calculate R-squared
rsq  =  1 - (rss/tss)
cat('The R-square of the test data is ', round(rsq,3), '\n')
```

```{r, warning=FALSE}
options(repr.plot.width=8, repr.plot.height=4)
my_data = as.data.frame(cbind(predicted = predicted,
                            observed = y_test))
# Plot predictions vs test data
ggplot(my_data,aes(predicted, observed)) + geom_point(color = "darkred", alpha = 0.5) + 
    geom_smooth(method=lm)+ ggtitle('Gradient Boosted Machines ') + ggtitle("Random Forest: Prediction vs Test Data") +
      xlab("Predecited Trip Duration in Minutes ") + ylab("Observed Trip Duration in Minutes") + 
        theme(plot.title = element_text(size=16,hjust = 0.5),
         axis.text.y = element_text(size=12), axis.text.x = element_text(size=12,hjust=.5),
         axis.title.x = element_text(size=14), axis.title.y = element_text(size=14))
```

# Conclusion

I was able to analyse a lot of information from the hypotheses I tested and even by building the predictive models. Cleaning the dataset and extracting features was also fun. I can make a lot of recommendations based on the hypothesis I tested. For instance, the citibike subscriptions should be marketed to the young demographic as they take more number of rides in general. Business would be slow during the winter months so inventory can be managed effectively and maybe repairs can be scheduled during this time. Demand is high during weekends so price can be surged whereas during weekdays discount offers should be pushed to drive users to the system. The prediction models I built can also be used to effectively manage inventory and also to predict sales for the future. 

A future scope for this could be to improve the predictive models and also perform more feature extraction. This can be extended to predicting demand during hours of the days. We can also predict the rider destination using the source and their previous ride patterns. All of these can be together used by the company to manage their inventory very effectively and cut costs and increase profits.